{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the EESSI project documentation! \u00b6 Quote What if there was a way to avoid having to install a broad range of scientific software from scratch on every HPC cluster or cloud instance you use or maintain, without compromising on performance? The European Environment for Scientific Software Installations (EESSI, pronounced as \"easy\") is a collaboration between different European partners in HPC community. The goal of this project is to build a common stack of scientific software installations for HPC systems and beyond, including laptops, personal workstations and cloud infrastructure. More details about the project are available in the different subsections: Project overview Filesystem layer Compatibility layer Software layer Pilot repository Project partners Contact info The EESSI project was presented at the HPC Knowledge Meeting in June 2020. Check the recording:","title":"Home"},{"location":"#welcome-to-the-eessi-project-documentation","text":"Quote What if there was a way to avoid having to install a broad range of scientific software from scratch on every HPC cluster or cloud instance you use or maintain, without compromising on performance? The European Environment for Scientific Software Installations (EESSI, pronounced as \"easy\") is a collaboration between different European partners in HPC community. The goal of this project is to build a common stack of scientific software installations for HPC systems and beyond, including laptops, personal workstations and cloud infrastructure. More details about the project are available in the different subsections: Project overview Filesystem layer Compatibility layer Software layer Pilot repository Project partners Contact info The EESSI project was presented at the HPC Knowledge Meeting in June 2020. Check the recording:","title":"Welcome to the EESSI project documentation!"},{"location":"compatibility_layer/","text":"Compatibility layer \u00b6 The middle layer of the EESSI project is the compatibility layer , which ensures that our scientific software stack is compatible with different client operating systems (different Linux distributions, macOS and even Windows via WSL ). For this we rely on Gentoo Prefix , by installing a limited set of Gentoo Linux packages in a non-standard location (a \"prefix\"), using Gentoo's package manager Portage . The compatible layer is maintained via our https://github.com/EESSI/compatibility-layer GitHub repository.","title":"Compatibility layer"},{"location":"compatibility_layer/#compatibility-layer","text":"The middle layer of the EESSI project is the compatibility layer , which ensures that our scientific software stack is compatible with different client operating systems (different Linux distributions, macOS and even Windows via WSL ). For this we rely on Gentoo Prefix , by installing a limited set of Gentoo Linux packages in a non-standard location (a \"prefix\"), using Gentoo's package manager Portage . The compatible layer is maintained via our https://github.com/EESSI/compatibility-layer GitHub repository.","title":"Compatibility layer"},{"location":"contact/","text":"Contact info \u00b6 For more information: visit our website https://www.eessi-hpc.org consult our documentation at https://eessi.github.io reach out to one of the project partners check out our GitHub repositories at https://github.com/EESSI follow us on Twitter: https://twitter.com/eessi_hpc A Slack channel is available for the EESSI community (an invitation is required to join).","title":"Contact info"},{"location":"contact/#contact-info","text":"For more information: visit our website https://www.eessi-hpc.org consult our documentation at https://eessi.github.io reach out to one of the project partners check out our GitHub repositories at https://github.com/EESSI follow us on Twitter: https://twitter.com/eessi_hpc A Slack channel is available for the EESSI community (an invitation is required to join).","title":"Contact info"},{"location":"filesystem_layer/","text":"Filesystem layer \u00b6 The bottom layer of the EESSI project is the filesystem layer , which is responsible for distributing the software stack. For this we rely on CernVM-FS (or CVMFS for short), a network file system used to distribute the software to the clients in a fast, reliable and scalable way. CVMFS was created over 10 years ago specifically for the purpose of globally distributing a large software stack. For the experiments at the Large Hadron Collider, it hosts several hundred million files and directories that are distributed to the order of hundred thousand client computers. The hierarchical structure with multiple caching layers (Stratum-0, Stratum-1's located at partner sites and local caching proxies) ensures good performance with limited resources. Redundancy is provided by using multiple Stratum-1's at various sites. Since CVMFS is based on the HTTP protocol, the ubiquitous Squid caching proxy can be leveraged to reduce server loads and improve performance at large installations (such as HPC clusters). Clients can easily mount the file system (read-only) via a FUSE (Filesystem in Userspace) module. For a (basic) introduction to CernVM-FS, see this presentation . Detailed information about how we configure CVMFS is available at https://github.com/EESSI/filesystem-layer .","title":"Filesystem layer"},{"location":"filesystem_layer/#filesystem-layer","text":"The bottom layer of the EESSI project is the filesystem layer , which is responsible for distributing the software stack. For this we rely on CernVM-FS (or CVMFS for short), a network file system used to distribute the software to the clients in a fast, reliable and scalable way. CVMFS was created over 10 years ago specifically for the purpose of globally distributing a large software stack. For the experiments at the Large Hadron Collider, it hosts several hundred million files and directories that are distributed to the order of hundred thousand client computers. The hierarchical structure with multiple caching layers (Stratum-0, Stratum-1's located at partner sites and local caching proxies) ensures good performance with limited resources. Redundancy is provided by using multiple Stratum-1's at various sites. Since CVMFS is based on the HTTP protocol, the ubiquitous Squid caching proxy can be leveraged to reduce server loads and improve performance at large installations (such as HPC clusters). Clients can easily mount the file system (read-only) via a FUSE (Filesystem in Userspace) module. For a (basic) introduction to CernVM-FS, see this presentation . Detailed information about how we configure CVMFS is available at https://github.com/EESSI/filesystem-layer .","title":"Filesystem layer"},{"location":"overview/","text":"Overview of the EESSI project \u00b6 Scope & Goals \u00b6 Through the EESSI project, we want to set up a shared stack of scientific software installations , and by doing so avoid a lot of duplicate work across HPC sites. For end users, we want to provide a uniform user experience with respect to available scientific software, regardless of which system they use. Our software stack should work on laptops, personal workstations, HPC clusters and in the cloud, which means we will need to support different CPUs, networks, GPUs, and so on. We hope to make this work for any Linux distribution and maybe even macOS and Windows via WSL , and a wide variety of CPU architectures (Intel, AMD, ARM, POWER, RISC-V). Of course we want to focus on the performance of the software, but also on automating the workflow for maintaining the software stack, thoroughly testing the installations, and collaborating efficiently. Inspiration \u00b6 The EESSI concept is heavily inspired by Compute Canada software stack, which is a shared software stack used on all 5 major national systems in Canada and a bunch of smaller ones. The design of the Compute Canada software stack is discussed in detail in the PEARC'19 paper \"Providing a Unified Software Environment for Canada\u2019s National Advanced Computing Centers\" . It has also been presented at the 5th EasyBuild User Meetings ( slides , recorded talk ), and is well documented . Layered structure \u00b6 The EESSI project consists of 3 layers. The bottom layer is the filesystem layer , which is responsible for distributing the software stack across clients. The middle layer is a compatibility layer , which ensures that the software stack is compatible with multiple different client operating systems. The top layer is the software layer , which contains the actual scientific software applications and their dependencies. The host OS still provides a couple of things, like drivers for network and GPU, support for shared filesystems like GPFS and Lustre, a resource manager like Slurm, and so on. Opportunities \u00b6 We hope to collaborate with interested parties across the HPC community, including HPC centres, vendors, consultancy companies and scientific software developers. Through our software stack, HPC users can seamlessly hop between sites, since the same software is available everywhere. We can leverage each others work with respect to providing tested and properly optimized scientific software installations more efficiently, and provide a platform for easy benchmarking of new systems. By working together with the developers of scientific software we can provide vetted installations for the broad HPC community. Challenges \u00b6 There are many challenges in an ambitious project like this, including (but probably not limited to): Finding time and manpower to get the software stack set up properly; Leveraging system sources like network interconnect (MPI & co), accelerators (GPUs), ...; Supporting CPU architectures other than x86_64, including ARM, POWER, RISC-V, ... Dealing with licensed software, like Intel tools, MATLAB, ANSYS, ...; Integration with resource managers (Slurm) and vendor provided software (Cray PE); Convincing HPC site admins to adopt EESSI; Current status \u00b6 (June 2020) We are actively working on a pilot setup that has a limited scope, and are organizing monthly meetings to discuss progress and next steps forward. Keep an eye on our GitHub repositories at https://github.com/EESSI and our Twitter feed .","title":"Project overview"},{"location":"overview/#overview-of-the-eessi-project","text":"","title":"Overview of the EESSI project"},{"location":"overview/#scope-goals","text":"Through the EESSI project, we want to set up a shared stack of scientific software installations , and by doing so avoid a lot of duplicate work across HPC sites. For end users, we want to provide a uniform user experience with respect to available scientific software, regardless of which system they use. Our software stack should work on laptops, personal workstations, HPC clusters and in the cloud, which means we will need to support different CPUs, networks, GPUs, and so on. We hope to make this work for any Linux distribution and maybe even macOS and Windows via WSL , and a wide variety of CPU architectures (Intel, AMD, ARM, POWER, RISC-V). Of course we want to focus on the performance of the software, but also on automating the workflow for maintaining the software stack, thoroughly testing the installations, and collaborating efficiently.","title":"Scope &amp; Goals"},{"location":"overview/#inspiration","text":"The EESSI concept is heavily inspired by Compute Canada software stack, which is a shared software stack used on all 5 major national systems in Canada and a bunch of smaller ones. The design of the Compute Canada software stack is discussed in detail in the PEARC'19 paper \"Providing a Unified Software Environment for Canada\u2019s National Advanced Computing Centers\" . It has also been presented at the 5th EasyBuild User Meetings ( slides , recorded talk ), and is well documented .","title":"Inspiration"},{"location":"overview/#layered-structure","text":"The EESSI project consists of 3 layers. The bottom layer is the filesystem layer , which is responsible for distributing the software stack across clients. The middle layer is a compatibility layer , which ensures that the software stack is compatible with multiple different client operating systems. The top layer is the software layer , which contains the actual scientific software applications and their dependencies. The host OS still provides a couple of things, like drivers for network and GPU, support for shared filesystems like GPFS and Lustre, a resource manager like Slurm, and so on.","title":"Layered structure"},{"location":"overview/#opportunities","text":"We hope to collaborate with interested parties across the HPC community, including HPC centres, vendors, consultancy companies and scientific software developers. Through our software stack, HPC users can seamlessly hop between sites, since the same software is available everywhere. We can leverage each others work with respect to providing tested and properly optimized scientific software installations more efficiently, and provide a platform for easy benchmarking of new systems. By working together with the developers of scientific software we can provide vetted installations for the broad HPC community.","title":"Opportunities"},{"location":"overview/#challenges","text":"There are many challenges in an ambitious project like this, including (but probably not limited to): Finding time and manpower to get the software stack set up properly; Leveraging system sources like network interconnect (MPI & co), accelerators (GPUs), ...; Supporting CPU architectures other than x86_64, including ARM, POWER, RISC-V, ... Dealing with licensed software, like Intel tools, MATLAB, ANSYS, ...; Integration with resource managers (Slurm) and vendor provided software (Cray PE); Convincing HPC site admins to adopt EESSI;","title":"Challenges"},{"location":"overview/#current-status","text":"(June 2020) We are actively working on a pilot setup that has a limited scope, and are organizing monthly meetings to discuss progress and next steps forward. Keep an eye on our GitHub repositories at https://github.com/EESSI and our Twitter feed .","title":"Current status"},{"location":"partners/","text":"Project partners \u00b6 Delft University of Technology (The Netherlands) \u00b6 Robbert Eggermont Koen Mulderij Dell Technologies (Europe) \u00b6 Walther Blom, High Education & Research Jaco van Dijk, Higher Education Eindhoven University of Technology \u00b6 Patrick Van Brakel Ghent University (Belgium) \u00b6 Kenneth Hoste, HPC-UGent HPCNow! (Spain) \u00b6 Oriol Mula Valls J\u00fclich Supercomputing Centre (Germany) \u00b6 Alan O'Cais University of Cambridge (United Kingdom) \u00b6 Mark Sharpley, Research Computing Services Division University of Groningen (The Netherlands) \u00b6 Bob Dr\u00f6ge, Center for Information Technology Henk-Jan Zilverberg, Center for Information Technology University of Twente (The Netherlands) \u00b6 Geert Jan Laanstra, Electrical Engineering, Mathematics and Computer Science (EEMCS) University of Oslo (Norway) \u00b6 Thomas R\u00f6blitz Vrije Universiteit Amsterdam (The Netherlands) \u00b6 Peter Stol SURF (The Netherlands) \u00b6 Caspar van Leeuwen Marco Verdicchio Bas van der Vlies","title":"Project partners"},{"location":"partners/#project-partners","text":"","title":"Project partners"},{"location":"partners/#delft-university-of-technology-the-netherlands","text":"Robbert Eggermont Koen Mulderij","title":"Delft University of Technology (The Netherlands)"},{"location":"partners/#dell-technologies-europe","text":"Walther Blom, High Education & Research Jaco van Dijk, Higher Education","title":"Dell Technologies (Europe)"},{"location":"partners/#eindhoven-university-of-technology","text":"Patrick Van Brakel","title":"Eindhoven University of Technology"},{"location":"partners/#ghent-university-belgium","text":"Kenneth Hoste, HPC-UGent","title":"Ghent University (Belgium)"},{"location":"partners/#hpcnow-spain","text":"Oriol Mula Valls","title":"HPCNow! (Spain)"},{"location":"partners/#julich-supercomputing-centre-germany","text":"Alan O'Cais","title":"J\u00fclich Supercomputing Centre (Germany)"},{"location":"partners/#university-of-cambridge-united-kingdom","text":"Mark Sharpley, Research Computing Services Division","title":"University of Cambridge (United Kingdom)"},{"location":"partners/#university-of-groningen-the-netherlands","text":"Bob Dr\u00f6ge, Center for Information Technology Henk-Jan Zilverberg, Center for Information Technology","title":"University of Groningen (The Netherlands)"},{"location":"partners/#university-of-twente-the-netherlands","text":"Geert Jan Laanstra, Electrical Engineering, Mathematics and Computer Science (EEMCS)","title":"University of Twente (The Netherlands)"},{"location":"partners/#university-of-oslo-norway","text":"Thomas R\u00f6blitz","title":"University of Oslo (Norway)"},{"location":"partners/#vrije-universiteit-amsterdam-the-netherlands","text":"Peter Stol","title":"Vrije Universiteit Amsterdam (The Netherlands)"},{"location":"partners/#surf-the-netherlands","text":"Caspar van Leeuwen Marco Verdicchio Bas van der Vlies","title":"SURF (The Netherlands)"},{"location":"pilot/","text":"Pilot software stack (2020.08) \u00b6 Caveats \u00b6 The current EESSI pilot software stack (version 2020.08) is the first iteration, and there are some known issues and limitations, please take these into account: First of all: the EESSI pilot software stack is NOT READY FOR PRODUCTION! Do not use it for production work, and be careful when testing it on production systems! The software is currently only installed for systems compatible with Intel Haswell processors. Installations tuned for other CPU architectures are not available yet (but are hopefully coming soon). There is no Lmod (spider) cache available yet for the environment module files included in the pilot repository, so module commands may be somewhat slow. The provided Open MPI installation ( OpenMPI/4.0.3-GCC-9.3.0 module) is not properly configured yet to use high-speed network interconnects. Accessing the EESSI pilot repository through Singularity \u00b6 The easiest way to access the EESSI pilot repository is by using Singularity. If Singularity is installed already, no admin privileges are required. No other software is needed either on the host. A container image is available in Docker Hub (see https://hub.docker.com/r/eessi/client-pilot ). It only contains a minimal operating system + the necessary packages to access the EESSI pilot repository through CernVM-FS. The container image can be used directly by Singularity (no prior download required), as follows: First, create some local directories in /tmp/$USER which will be bind mounted in the container: mkdir -p /tmp/ $USER / { var-lib-cvmfs,var-run-cvmfs,home } These provides space for the CernVM-FS cache, and an empty home directory to use in the container. Set the $SINGULARITY_BIND and $SINGULARITY_HOME environment variables to configure Singularity: export SINGULARITY_BIND = \"/tmp/ $USER /var-run-cvmfs:/var/run/cvmfs,/tmp/ $USER /var-lib-cvmfs:/var/lib/cvmfs\" export SINGULARITY_HOME = \"/tmp/ $USER /home:/home/ $USER \" Start the container using singularity shell , using --fusemount to mount the EESSI config and pilot repositories (using the cvmfs2 command that is included in the container image): export EESSI_CONFIG = \"container:cvmfs2 cvmfs-config.eessi-hpc.org /cvmfs/cvmfs-config.eessi-hpc.org\" export EESSI_PILOT = \"container:cvmfs2 pilot.eessi-hpc.org /cvmfs/pilot.eessi-hpc.org\" singularity shell --fusemount \" $EESSI_CONFIG \" --fusemount \" $EESSI_PILOT \" docker://eessi/client-pilot:centos7-2020.08 This should give you a shell in the container, where the EESSI config and pilot repositories are mounted: $ singularity shell --fusemount \"$EESSI_CONFIG\" --fusemount \"$EESSI_PILOT\" docker://eessi/client-pilot:centos7-2020.08 INFO: Using cached SIF image CernVM-FS: pre-mounted on file descriptor 3 CernVM-FS: pre-mounted on file descriptor 3 CernVM-FS: loading Fuse module... done CernVM-FS: loading Fuse module... done Singularity> It is possible that you see some scary looking warnings, but those can be ignored for now. To verify that things are working, check the contents of the /cvmfs/pilot.eessi-hpc.org/2020.08 directory: Singularity> ls /cvmfs/pilot.eessi-hpc.org/2020.08 compat init software Setting up the EESSI environment \u00b6 Once you have the EESSI pilot repository mounted, you can set up the environment by sourcing the provided init script: source /cvmfs/pilot.eessi-hpc.org/2020.08/init/bash If all goes well, you should see output like this: Singularity> source /cvmfs/pilot.eessi-hpc.org/2020.08/init/bash Found EESSI pilot repo @ /cvmfs/pilot.eessi-hpc.org/2020.08! Derived subdirectory for software layer: x86_64/intel/haswell Using x86_64/intel/haswell subdirectory for software layer ( HARDCODED ) Initializing Lmod... Prepending /cvmfs/pilot.eessi-hpc.org/2020.08/software/x86_64/intel/haswell/modules/all to $MODULEPATH ... Environment set up to use EESSI pilot software stack, have fun! [ EESSI pilot 2020 .08 ] $ Now you're all set up! Go ahead and explore the software stack using \" module avail \", and go wild with testing the available software installations! Testing the EESSI pilot software stack \u00b6 Please test the EESSI pilot software stack as you see fit: running simple commands, performing small calculations or running small benchmarks, etc. Test scripts that have been verified to work correctly using the pilot software stack are available at https://github.com/EESSI/software-layer/tree/master/tests . Giving feedback or reporting problems \u00b6 Any feedback is welcome, and questions or problems reports are welcome as well, through one of the EESSI communication channels: ( preferred! ) EESSI software-layer GitHub repository: https://github.com/EESSI/software-layer/issues EESSI mailing list ( eessi@list.rug.nl ) EESSI Slack: https://eessi-hpc.slack.com (get an invite via https://www.eessi-hpc.org/join ) monthly EESSI meetings (first Thursday of the month at 2pm CEST) Available software \u00b6 (last update: Sept 3rd 2020) [EESSI pilot 2020.08] $ module avail -------------------------------------- /cvmfs/pilot.eessi-hpc.org/2020.08/software/x86_64/intel/haswell/modules/all -------------------------------------- Autoconf/2.69-GCCcore-9.3.0 (D) OpenBLAS/0.3.9-GCC-9.3.0 (D) help2man/1.47.4 Automake/1.16.1-GCCcore-9.3.0 (D) OpenFOAM/v2006-foss-2020a help2man/1.47.12-GCCcore-9.3.0 Autotools/20180311-GCCcore-9.3.0 (D) OpenFOAM/8-foss-2020a (D) hwloc/2.2.0-GCCcore-9.3.0 (D) Bison/3.3.2 OpenMPI/4.0.3-GCC-9.3.0 intltool/0.51.0-GCCcore-9.3.0 (D) Bison/3.5.3-GCCcore-9.3.0 PCRE/8.44-GCCcore-9.3.0 (D) libGLU/9.0.1-GCCcore-9.3.0 (D) Bison/3.5.3 PCRE2/10.34-GCCcore-9.3.0 (D) libdrm/2.4.100-GCCcore-9.3.0 (D) Boost/1.72.0-gompi-2020a ParaView/5.8.0-foss-2020a-Python-3.8.2-mpi libevent/2.1.11-GCCcore-9.3.0 (D) CGAL/4.14.3-gompi-2020a-Python-3.8.2 (D) Perl/5.30.2-GCCcore-9.3.0 libffi/3.3-GCCcore-9.3.0 (D) CMake/3.16.4-GCCcore-9.3.0 (D) Python/2.7.18-GCCcore-9.3.0 libglvnd/1.2.0-GCCcore-9.3.0 (D) DBus/1.13.12-GCCcore-9.3.0 (D) Python/3.8.2-GCCcore-9.3.0 (D) libiconv/1.16-GCCcore-9.3.0 (D) Doxygen/1.8.17-GCCcore-9.3.0 (D) Qt5/5.14.1-GCCcore-9.3.0 (D) libjpeg-turbo/2.0.4-GCCcore-9.3.0 (D) EasyBuild/4.2.2 SCOTCH/6.0.9-gompi-2020a libpciaccess/0.16-GCCcore-9.3.0 (D) EasyBuild/20200831-dev (D) SQLite/3.31.1-GCCcore-9.3.0 (D) libpng/1.6.37-GCCcore-9.3.0 (D) Eigen/3.3.7-GCCcore-9.3.0 ScaLAPACK/2.1.0-gompi-2020a (D) libreadline/8.0-GCCcore-9.3.0 FFTW/3.3.8-gompi-2020a SciPy-bundle/2020.03-foss-2020a-Python-3.8.2 libtool/2.4.6-GCCcore-9.3.0 (D) FFmpeg/4.2.2-GCCcore-9.3.0 (D) Szip/2.1.1-GCCcore-9.3.0 (D) libunwind/1.3.1-GCCcore-9.3.0 (D) FriBidi/1.0.9-GCCcore-9.3.0 (D) Tcl/8.6.10-GCCcore-9.3.0 (D) libxml2/2.9.10-GCCcore-9.3.0 (D) GCC/9.3.0 UCX/1.8.0-GCCcore-9.3.0 (D) lz4/1.9.2-GCCcore-9.3.0 (D) GCCcore/9.3.0 X11/20200222-GCCcore-9.3.0 (D) ncurses/6.1 GLib/2.64.1-GCCcore-9.3.0 (D) XZ/5.2.5-GCCcore-9.3.0 (D) ncurses/6.2-GCCcore-9.3.0 GMP/6.2.0-GCCcore-9.3.0 (D) Yasm/1.3.0-GCCcore-9.3.0 (D) netCDF/4.7.4-gompi-2020a GROMACS/2020.1-foss-2020a-Python-3.8.2 (D) binutils/2.34-GCCcore-9.3.0 networkx/2.4-foss-2020a-Python-3.8.2 HDF5/1.10.6-gompi-2020a binutils/2.34 numactl/2.0.13-GCCcore-9.3.0 (D) JasPer/2.0.14-GCCcore-9.3.0 (D) bzip2/1.0.8-GCCcore-9.3.0 (D) pkg-config/0.29.2-GCCcore-9.3.0 (D) LAME/3.100-GCCcore-9.3.0 (D) cURL/7.69.1-GCCcore-9.3.0 (D) pybind11/2.4.3-GCCcore-9.3.0-Python-3.8.2 (D) LLVM/9.0.1-GCCcore-9.3.0 (D) double-conversion/3.1.5-GCCcore-9.3.0 (D) re2c/1.3-GCCcore-9.3.0 (D) M4/1.4.18-GCCcore-9.3.0 expat/2.2.9-GCCcore-9.3.0 scikit-build/0.10.0-foss-2020a-Python-3.8.2 (D) M4/1.4.18 (D) flex/2.6.4-GCCcore-9.3.0 snappy/1.1.8-GCCcore-9.3.0 (D) METIS/5.1.0-GCCcore-9.3.0 (D) flex/2.6.4 (D) util-linux/2.35-GCCcore-9.3.0 (D) MPFR/4.0.2-GCCcore-9.3.0 (D) fontconfig/2.13.92-GCCcore-9.3.0 (D) x264/20191217-GCCcore-9.3.0 (D) Mako/1.1.2-GCCcore-9.3.0 (D) foss/2020a (D) x265/3.3-GCCcore-9.3.0 (D) Mesa/20.0.2-GCCcore-9.3.0 (D) freetype/2.10.1-GCCcore-9.3.0 (D) xorg-macros/1.19.2-GCCcore-9.3.0 (D) Meson/0.55.1-GCCcore-9.3.0-Python-3.8.2 (D) gettext/0.20.1-GCCcore-9.3.0 zlib/1.2.11-GCCcore-9.3.0 NASM/2.14.02-GCCcore-9.3.0 (D) gettext/0.20.1 (D) zlib/1.2.11 (D) NSPR/4.25-GCCcore-9.3.0 (D) gompi/2020a (D) zstd/1.4.4-GCCcore-9.3.0 (D) NSS/3.51-GCCcore-9.3.0 (D) gperf/3.1-GCCcore-9.3.0 (D) Ninja/1.10.0-GCCcore-9.3.0 (D) gzip/1.10-GCCcore-9.3.0 (D) Build host \u00b6 CentOS 7.8.2003 Intel Xeon CPU E5-2680 ( haswell ) Singularity container with CentOS 7.8.2003 + cvmfs 2.7.3 EasyBuild configuration \u00b6 The latest develop version of EasyBuild was used, all changes required to install the software in the 2020.08 version of the pilot repository will be included in the upcoming release of EasyBuild v4.3.0. $ eb --show-config # # Current EasyBuild configuration # (C: command line argument, D: default value, E: environment variable, F: configuration file) # buildpath (E) = /tmp/easybuild/build containerpath (E) = /tmp/easybuild/containers debug (E) = True ignore-osdeps (E) = True installpath (E) = /cvmfs/pilot.eessi-hpc.org/2020.08/software/x86_64/intel/haswell packagepath (E) = /tmp/easybuild/packages prefix (E) = /tmp/easybuild repositorypath (E) = /tmp/easybuild/ebfiles_repo robot-paths (D) = /home/eessi/easybuild-easyconfigs/easybuild/easyconfigs rpath (E) = True sourcepath (E) = /tmp/easybuild/sources sysroot (E) = /cvmfs/pilot.eessi-hpc.org/2020.08/compat/x86_64 trace (E) = True","title":"Pilot repository"},{"location":"pilot/#pilot-software-stack-202008","text":"","title":"Pilot software stack (2020.08)"},{"location":"pilot/#caveats","text":"The current EESSI pilot software stack (version 2020.08) is the first iteration, and there are some known issues and limitations, please take these into account: First of all: the EESSI pilot software stack is NOT READY FOR PRODUCTION! Do not use it for production work, and be careful when testing it on production systems! The software is currently only installed for systems compatible with Intel Haswell processors. Installations tuned for other CPU architectures are not available yet (but are hopefully coming soon). There is no Lmod (spider) cache available yet for the environment module files included in the pilot repository, so module commands may be somewhat slow. The provided Open MPI installation ( OpenMPI/4.0.3-GCC-9.3.0 module) is not properly configured yet to use high-speed network interconnects.","title":"Caveats"},{"location":"pilot/#accessing-the-eessi-pilot-repository-through-singularity","text":"The easiest way to access the EESSI pilot repository is by using Singularity. If Singularity is installed already, no admin privileges are required. No other software is needed either on the host. A container image is available in Docker Hub (see https://hub.docker.com/r/eessi/client-pilot ). It only contains a minimal operating system + the necessary packages to access the EESSI pilot repository through CernVM-FS. The container image can be used directly by Singularity (no prior download required), as follows: First, create some local directories in /tmp/$USER which will be bind mounted in the container: mkdir -p /tmp/ $USER / { var-lib-cvmfs,var-run-cvmfs,home } These provides space for the CernVM-FS cache, and an empty home directory to use in the container. Set the $SINGULARITY_BIND and $SINGULARITY_HOME environment variables to configure Singularity: export SINGULARITY_BIND = \"/tmp/ $USER /var-run-cvmfs:/var/run/cvmfs,/tmp/ $USER /var-lib-cvmfs:/var/lib/cvmfs\" export SINGULARITY_HOME = \"/tmp/ $USER /home:/home/ $USER \" Start the container using singularity shell , using --fusemount to mount the EESSI config and pilot repositories (using the cvmfs2 command that is included in the container image): export EESSI_CONFIG = \"container:cvmfs2 cvmfs-config.eessi-hpc.org /cvmfs/cvmfs-config.eessi-hpc.org\" export EESSI_PILOT = \"container:cvmfs2 pilot.eessi-hpc.org /cvmfs/pilot.eessi-hpc.org\" singularity shell --fusemount \" $EESSI_CONFIG \" --fusemount \" $EESSI_PILOT \" docker://eessi/client-pilot:centos7-2020.08 This should give you a shell in the container, where the EESSI config and pilot repositories are mounted: $ singularity shell --fusemount \"$EESSI_CONFIG\" --fusemount \"$EESSI_PILOT\" docker://eessi/client-pilot:centos7-2020.08 INFO: Using cached SIF image CernVM-FS: pre-mounted on file descriptor 3 CernVM-FS: pre-mounted on file descriptor 3 CernVM-FS: loading Fuse module... done CernVM-FS: loading Fuse module... done Singularity> It is possible that you see some scary looking warnings, but those can be ignored for now. To verify that things are working, check the contents of the /cvmfs/pilot.eessi-hpc.org/2020.08 directory: Singularity> ls /cvmfs/pilot.eessi-hpc.org/2020.08 compat init software","title":"Accessing the EESSI pilot repository through Singularity"},{"location":"pilot/#setting-up-the-eessi-environment","text":"Once you have the EESSI pilot repository mounted, you can set up the environment by sourcing the provided init script: source /cvmfs/pilot.eessi-hpc.org/2020.08/init/bash If all goes well, you should see output like this: Singularity> source /cvmfs/pilot.eessi-hpc.org/2020.08/init/bash Found EESSI pilot repo @ /cvmfs/pilot.eessi-hpc.org/2020.08! Derived subdirectory for software layer: x86_64/intel/haswell Using x86_64/intel/haswell subdirectory for software layer ( HARDCODED ) Initializing Lmod... Prepending /cvmfs/pilot.eessi-hpc.org/2020.08/software/x86_64/intel/haswell/modules/all to $MODULEPATH ... Environment set up to use EESSI pilot software stack, have fun! [ EESSI pilot 2020 .08 ] $ Now you're all set up! Go ahead and explore the software stack using \" module avail \", and go wild with testing the available software installations!","title":"Setting up the EESSI environment"},{"location":"pilot/#testing-the-eessi-pilot-software-stack","text":"Please test the EESSI pilot software stack as you see fit: running simple commands, performing small calculations or running small benchmarks, etc. Test scripts that have been verified to work correctly using the pilot software stack are available at https://github.com/EESSI/software-layer/tree/master/tests .","title":"Testing the EESSI pilot software stack"},{"location":"pilot/#giving-feedback-or-reporting-problems","text":"Any feedback is welcome, and questions or problems reports are welcome as well, through one of the EESSI communication channels: ( preferred! ) EESSI software-layer GitHub repository: https://github.com/EESSI/software-layer/issues EESSI mailing list ( eessi@list.rug.nl ) EESSI Slack: https://eessi-hpc.slack.com (get an invite via https://www.eessi-hpc.org/join ) monthly EESSI meetings (first Thursday of the month at 2pm CEST)","title":"Giving feedback or reporting problems"},{"location":"pilot/#available-software","text":"(last update: Sept 3rd 2020) [EESSI pilot 2020.08] $ module avail -------------------------------------- /cvmfs/pilot.eessi-hpc.org/2020.08/software/x86_64/intel/haswell/modules/all -------------------------------------- Autoconf/2.69-GCCcore-9.3.0 (D) OpenBLAS/0.3.9-GCC-9.3.0 (D) help2man/1.47.4 Automake/1.16.1-GCCcore-9.3.0 (D) OpenFOAM/v2006-foss-2020a help2man/1.47.12-GCCcore-9.3.0 Autotools/20180311-GCCcore-9.3.0 (D) OpenFOAM/8-foss-2020a (D) hwloc/2.2.0-GCCcore-9.3.0 (D) Bison/3.3.2 OpenMPI/4.0.3-GCC-9.3.0 intltool/0.51.0-GCCcore-9.3.0 (D) Bison/3.5.3-GCCcore-9.3.0 PCRE/8.44-GCCcore-9.3.0 (D) libGLU/9.0.1-GCCcore-9.3.0 (D) Bison/3.5.3 PCRE2/10.34-GCCcore-9.3.0 (D) libdrm/2.4.100-GCCcore-9.3.0 (D) Boost/1.72.0-gompi-2020a ParaView/5.8.0-foss-2020a-Python-3.8.2-mpi libevent/2.1.11-GCCcore-9.3.0 (D) CGAL/4.14.3-gompi-2020a-Python-3.8.2 (D) Perl/5.30.2-GCCcore-9.3.0 libffi/3.3-GCCcore-9.3.0 (D) CMake/3.16.4-GCCcore-9.3.0 (D) Python/2.7.18-GCCcore-9.3.0 libglvnd/1.2.0-GCCcore-9.3.0 (D) DBus/1.13.12-GCCcore-9.3.0 (D) Python/3.8.2-GCCcore-9.3.0 (D) libiconv/1.16-GCCcore-9.3.0 (D) Doxygen/1.8.17-GCCcore-9.3.0 (D) Qt5/5.14.1-GCCcore-9.3.0 (D) libjpeg-turbo/2.0.4-GCCcore-9.3.0 (D) EasyBuild/4.2.2 SCOTCH/6.0.9-gompi-2020a libpciaccess/0.16-GCCcore-9.3.0 (D) EasyBuild/20200831-dev (D) SQLite/3.31.1-GCCcore-9.3.0 (D) libpng/1.6.37-GCCcore-9.3.0 (D) Eigen/3.3.7-GCCcore-9.3.0 ScaLAPACK/2.1.0-gompi-2020a (D) libreadline/8.0-GCCcore-9.3.0 FFTW/3.3.8-gompi-2020a SciPy-bundle/2020.03-foss-2020a-Python-3.8.2 libtool/2.4.6-GCCcore-9.3.0 (D) FFmpeg/4.2.2-GCCcore-9.3.0 (D) Szip/2.1.1-GCCcore-9.3.0 (D) libunwind/1.3.1-GCCcore-9.3.0 (D) FriBidi/1.0.9-GCCcore-9.3.0 (D) Tcl/8.6.10-GCCcore-9.3.0 (D) libxml2/2.9.10-GCCcore-9.3.0 (D) GCC/9.3.0 UCX/1.8.0-GCCcore-9.3.0 (D) lz4/1.9.2-GCCcore-9.3.0 (D) GCCcore/9.3.0 X11/20200222-GCCcore-9.3.0 (D) ncurses/6.1 GLib/2.64.1-GCCcore-9.3.0 (D) XZ/5.2.5-GCCcore-9.3.0 (D) ncurses/6.2-GCCcore-9.3.0 GMP/6.2.0-GCCcore-9.3.0 (D) Yasm/1.3.0-GCCcore-9.3.0 (D) netCDF/4.7.4-gompi-2020a GROMACS/2020.1-foss-2020a-Python-3.8.2 (D) binutils/2.34-GCCcore-9.3.0 networkx/2.4-foss-2020a-Python-3.8.2 HDF5/1.10.6-gompi-2020a binutils/2.34 numactl/2.0.13-GCCcore-9.3.0 (D) JasPer/2.0.14-GCCcore-9.3.0 (D) bzip2/1.0.8-GCCcore-9.3.0 (D) pkg-config/0.29.2-GCCcore-9.3.0 (D) LAME/3.100-GCCcore-9.3.0 (D) cURL/7.69.1-GCCcore-9.3.0 (D) pybind11/2.4.3-GCCcore-9.3.0-Python-3.8.2 (D) LLVM/9.0.1-GCCcore-9.3.0 (D) double-conversion/3.1.5-GCCcore-9.3.0 (D) re2c/1.3-GCCcore-9.3.0 (D) M4/1.4.18-GCCcore-9.3.0 expat/2.2.9-GCCcore-9.3.0 scikit-build/0.10.0-foss-2020a-Python-3.8.2 (D) M4/1.4.18 (D) flex/2.6.4-GCCcore-9.3.0 snappy/1.1.8-GCCcore-9.3.0 (D) METIS/5.1.0-GCCcore-9.3.0 (D) flex/2.6.4 (D) util-linux/2.35-GCCcore-9.3.0 (D) MPFR/4.0.2-GCCcore-9.3.0 (D) fontconfig/2.13.92-GCCcore-9.3.0 (D) x264/20191217-GCCcore-9.3.0 (D) Mako/1.1.2-GCCcore-9.3.0 (D) foss/2020a (D) x265/3.3-GCCcore-9.3.0 (D) Mesa/20.0.2-GCCcore-9.3.0 (D) freetype/2.10.1-GCCcore-9.3.0 (D) xorg-macros/1.19.2-GCCcore-9.3.0 (D) Meson/0.55.1-GCCcore-9.3.0-Python-3.8.2 (D) gettext/0.20.1-GCCcore-9.3.0 zlib/1.2.11-GCCcore-9.3.0 NASM/2.14.02-GCCcore-9.3.0 (D) gettext/0.20.1 (D) zlib/1.2.11 (D) NSPR/4.25-GCCcore-9.3.0 (D) gompi/2020a (D) zstd/1.4.4-GCCcore-9.3.0 (D) NSS/3.51-GCCcore-9.3.0 (D) gperf/3.1-GCCcore-9.3.0 (D) Ninja/1.10.0-GCCcore-9.3.0 (D) gzip/1.10-GCCcore-9.3.0 (D)","title":"Available software"},{"location":"pilot/#build-host","text":"CentOS 7.8.2003 Intel Xeon CPU E5-2680 ( haswell ) Singularity container with CentOS 7.8.2003 + cvmfs 2.7.3","title":"Build host"},{"location":"pilot/#easybuild-configuration","text":"The latest develop version of EasyBuild was used, all changes required to install the software in the 2020.08 version of the pilot repository will be included in the upcoming release of EasyBuild v4.3.0. $ eb --show-config # # Current EasyBuild configuration # (C: command line argument, D: default value, E: environment variable, F: configuration file) # buildpath (E) = /tmp/easybuild/build containerpath (E) = /tmp/easybuild/containers debug (E) = True ignore-osdeps (E) = True installpath (E) = /cvmfs/pilot.eessi-hpc.org/2020.08/software/x86_64/intel/haswell packagepath (E) = /tmp/easybuild/packages prefix (E) = /tmp/easybuild repositorypath (E) = /tmp/easybuild/ebfiles_repo robot-paths (D) = /home/eessi/easybuild-easyconfigs/easybuild/easyconfigs rpath (E) = True sourcepath (E) = /tmp/easybuild/sources sysroot (E) = /cvmfs/pilot.eessi-hpc.org/2020.08/compat/x86_64 trace (E) = True","title":"EasyBuild configuration"},{"location":"software_layer/","text":"Software layer \u00b6 The top layer of the EESSI project is the software layer , which provides the actual scientific software installations. To install the software we include in our stack, we use EasyBuild , a framework for installing scientific software on HPC systems. These installations are optimized for a particular system architecture (specific CPU and GPU generation). To access these software installation we provide environment module files and use Lmod , a modern environment modules tool which has been widely adopted in the HPC community in recent years. We leverage the archspec Python library to automatically select the best suited part of the software stack for a particular host, based on its system architecture. The software layer is maintained through our https://github.com/EESSI/software-layer GitHub repository.","title":"Software layer"},{"location":"software_layer/#software-layer","text":"The top layer of the EESSI project is the software layer , which provides the actual scientific software installations. To install the software we include in our stack, we use EasyBuild , a framework for installing scientific software on HPC systems. These installations are optimized for a particular system architecture (specific CPU and GPU generation). To access these software installation we provide environment module files and use Lmod , a modern environment modules tool which has been widely adopted in the HPC community in recent years. We leverage the archspec Python library to automatically select the best suited part of the software stack for a particular host, based on its system architecture. The software layer is maintained through our https://github.com/EESSI/software-layer GitHub repository.","title":"Software layer"}]}